import logging
from datetime import datetime
from pathlib import Path

import pandas as pd

from back.scripts.communities.communities_selector import CommunitiesSelector
from back.scripts.datasets.communities_financial_accounts import FinancialAccounts
from back.scripts.datasets.datagouv_catalog import DataGouvCatalog
from back.scripts.datasets.datagouv_searcher import (
    DataGouvSearcher,
    remove_same_dataset_formats,
)
from back.scripts.datasets.declaration_interet import DeclaInteretWorkflow
from back.scripts.datasets.elected_officials import ElectedOfficialsWorkflow
from back.scripts.datasets.marches import MarchesPublicsWorkflow
from back.scripts.datasets.single_urls_builder import SingleUrlsBuilder
from back.scripts.datasets.sirene import SireneWorkflow
from back.scripts.datasets.topic_aggregator import TopicAggregator
from back.scripts.utils.config import get_project_data_path
from back.scripts.utils.dataframe_operation import (
    correct_format_from_url,
    sort_by_format_priorities,
)
from back.scripts.utils.datagouv_api import select_implemented_formats


class WorkflowManager:
    def __init__(self, args, config):
        self.args = args
        self.config = config
        self.logger = logging.getLogger(__name__)

        self.source_folder = get_project_data_path()
        self.source_folder.mkdir(exist_ok=True, parents=True)

    def run_workflow(self):
        self.logger.info("Workflow started.")
        DataGouvCatalog(self.config["datagouv_catalog"]).run()
        MarchesPublicsWorkflow.from_config(self.config["marches_publics"]).run()
        FinancialAccounts(self.config["financial_accounts"]).run()
        ElectedOfficialsWorkflow(self.config["elected_officials"]).run()
        SireneWorkflow(self.config["sirene"]).run()
        DeclaInteretWorkflow(self.config["declarations_interet"]).run()
        self._run_subvention_and_marche()

        self.logger.info("Workflow completed.")

    def _run_subvention_and_marche(self):
        # If communities files are already generated, check the age
        self.check_file_age(self.config["file_age_to_check"])

        communities_selector = self.initialize_communities_scope()

        # Loop through the topics defined in the config, e.g. marches publics or subventions.
        for topic, topic_config in self.config["search"].items():
            # Process each topic to get files in scope and datafiles
            self.process_topic(communities_selector, topic, topic_config)

    def check_file_age(self, config):
        """
        Check file age and log a warning if file is too aged according to config.yaml file, section: file_age_to_check
        """
        max_age_in_days = config["age"]

        for filename, filepath in config["files"].items():
            if Path(filepath).exists():
                filepath = Path(filepath)
                last_modified = datetime.fromtimestamp(filepath.stat().st_mtime)
                age_in_days = (datetime.now() - last_modified).days
                self.logger.info(
                    f"Found: {filename} at {filepath}, last update: {last_modified}, age: {age_in_days} days"
                )

                if age_in_days > max_age_in_days:
                    self.logger.warning(
                        f"{filename} file is older than {max_age_in_days} days. It is advised to refresh your data."
                    )

    def initialize_communities_scope(self):
        self.logger.info("Initializing communities scope.")
        # Initialize CommunitiesSelector with the config and select communities
        config = self.config["communities"] | {"sirene": self.config["sirene"]}
        communities_selector = CommunitiesSelector(config)

        self.logger.info("Communities scope initialized.")
        return communities_selector

    def process_topic(self, communities_selector, topic, topic_config):
        self.logger.info(f"Processing topic {topic}.")
        topic_files_in_scope = None

        if topic_config["source"] == "multiple":
            # Find multiple datafiles from datagouv
            config = self.config["datagouv"]
            config["datagouv_api"] = self.config["datagouv_api"]
            datagouv_searcher = DataGouvSearcher(communities_selector, config)
            datagouv_topic_files_in_scope = datagouv_searcher.select_datasets(topic_config)

            # Find single datafiles from single urls (standalone datasources outside of datagouv)
            single_urls_builder = SingleUrlsBuilder(communities_selector)
            single_urls_topic_files_in_scope = single_urls_builder.get_datafiles(topic_config)

            # Concatenate both datafiles lists into one
            topic_files_in_scope = (
                pd.concat(
                    [datagouv_topic_files_in_scope, single_urls_topic_files_in_scope],
                    ignore_index=True,
                )
                .dropna(subset=["url"])
                .pipe(correct_format_from_url)
                .pipe(sort_by_format_priorities)
                .drop_duplicates(subset=["url"], keep="first")
                .pipe(remove_same_dataset_formats)
                .pipe(select_implemented_formats)
            )

            topic_agg = TopicAggregator(
                topic_files_in_scope, topic, topic_config, self.config["datafile_loader"]
            )
            topic_agg.run()

            return topic_files_in_scope, topic_agg.aggregated_dataset
